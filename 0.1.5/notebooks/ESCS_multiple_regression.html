
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Bayesian Multiple Regression Example &#8212; Bambi 0.4.1 documentation</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "document", "processHtmlClass": "math|output_area"}}</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bayesian Workflow Example (Police Officer’s Dilemma)" href="shooter_crossed_random_ANOVA.html" />
    <link rel="prev" title="Bayesian/Frequentist Tutorial" href="Bayesian_Frequentist_Tutorial.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../index.html">
<p class="title">Bambi</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../getting_started.html">
  Getting Started
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../examples.html">
  Examples
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api_reference.html">
  API Reference
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/bambinos/bambi" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://pypi.org/project/bambi/" rel="noopener" target="_blank" title="PyPi">
            <span><i class="fas fa-box"></i></span>
            <label class="sr-only">PyPi</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>

  
  <p style="margin-top: 1em; margin-bottom: 0;">
    <strong>
    You're reading an old version of this documentation (v. 0.1.5).<br>
    If you want up-to-date information, please have a look at <a href="../../0.2.0/index.html">latest</a>.
  </strong>
  </p>
  

  

<nav class="bd-links" id="bd-docs-nav" aria-label="Versions navigation">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
        <li class="toctree-l1 has-children">
            <p class="caption"><span class="caption-text">Version</span></p>
            <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
            <label for="toctree-checkbox-1"> <i class="fas fa-chevron-down"></i></label>

            <ul>
                  
                  <li><a href="../../master/index.html"> Development </a></li>
                  

                

                  
                    <li><a href="../../0.2.0/index.html">0.2.0 (latest)</a></li>
                  

                

                

                  
                    <li class="toctree-l2 current active"><a href="ESCS_multiple_regression.html">0.1.5</a></li>
                  

                

            </ul>
        </li>
        </ul>
    </div>
  </nav>


            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#Load-and-examine-Eugene-Springfield-community-sample-data">
   Load and examine Eugene-Springfield community sample data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#Specify-model-and-examine-priors">
   Specify model and examine priors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#Examine-the-model-results">
   Examine the model results
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#Summarize-effects-on-partial-correlation-scale">
   Summarize effects on partial correlation scale
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#Relative-importance:-Which-predictors-have-the-strongest-effects-(defined-in-terms-of-partial-\eta^2)?">
   Relative importance: Which predictors have the strongest effects (defined in terms of partial
   <span class="math notranslate nohighlight">
    \(\eta^2\)
   </span>
   )?
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="Bayesian-Multiple-Regression-Example">
<h1>Bayesian Multiple Regression Example<a class="headerlink" href="#Bayesian-Multiple-Regression-Example" title="Permalink to this headline">¶</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Author: jake-westfall</span>
<span class="c1"># Created At: Aug 29, 2016</span>
<span class="c1"># Last Run: Mar 27, 2019</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">bambi</span> <span class="k">as</span> <span class="nn">bmb</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
<div class="section" id="Load-and-examine-Eugene-Springfield-community-sample-data">
<h2>Load and examine Eugene-Springfield community sample data<a class="headerlink" href="#Load-and-examine-Eugene-Springfield-community-sample-data" title="Permalink to this headline">¶</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/ESCS.csv&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">(),</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>drugs</th>
      <th>n</th>
      <th>e</th>
      <th>o</th>
      <th>a</th>
      <th>c</th>
      <th>hones</th>
      <th>emoti</th>
      <th>extra</th>
      <th>agree</th>
      <th>consc</th>
      <th>openn</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2.21</td>
      <td>80.04</td>
      <td>106.52</td>
      <td>113.87</td>
      <td>124.63</td>
      <td>124.23</td>
      <td>3.89</td>
      <td>3.18</td>
      <td>3.21</td>
      <td>3.13</td>
      <td>3.57</td>
      <td>3.41</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.65</td>
      <td>23.21</td>
      <td>19.88</td>
      <td>21.12</td>
      <td>16.67</td>
      <td>18.69</td>
      <td>0.45</td>
      <td>0.46</td>
      <td>0.53</td>
      <td>0.47</td>
      <td>0.44</td>
      <td>0.52</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.00</td>
      <td>23.00</td>
      <td>42.00</td>
      <td>51.00</td>
      <td>63.00</td>
      <td>44.00</td>
      <td>2.56</td>
      <td>1.47</td>
      <td>1.62</td>
      <td>1.59</td>
      <td>2.00</td>
      <td>1.28</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.71</td>
      <td>65.75</td>
      <td>93.00</td>
      <td>101.00</td>
      <td>115.00</td>
      <td>113.00</td>
      <td>3.59</td>
      <td>2.88</td>
      <td>2.84</td>
      <td>2.84</td>
      <td>3.31</td>
      <td>3.06</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>2.14</td>
      <td>76.00</td>
      <td>107.00</td>
      <td>112.00</td>
      <td>126.00</td>
      <td>125.00</td>
      <td>3.88</td>
      <td>3.19</td>
      <td>3.22</td>
      <td>3.16</td>
      <td>3.56</td>
      <td>3.44</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2.64</td>
      <td>93.00</td>
      <td>120.00</td>
      <td>129.00</td>
      <td>136.00</td>
      <td>136.00</td>
      <td>4.20</td>
      <td>3.47</td>
      <td>3.56</td>
      <td>3.44</td>
      <td>3.84</td>
      <td>3.75</td>
    </tr>
    <tr>
      <th>max</th>
      <td>4.29</td>
      <td>163.00</td>
      <td>158.00</td>
      <td>174.00</td>
      <td>171.00</td>
      <td>180.00</td>
      <td>4.94</td>
      <td>4.62</td>
      <td>4.75</td>
      <td>4.44</td>
      <td>4.75</td>
      <td>4.72</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>It’s always a good idea to start off with some basic plotting. Here’s what our outcome variable ‘drugs’ (some index of self-reported illegal drug use) looks like:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;drugs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_ESCS_multiple_regression_6_0.png" src="../_images/notebooks_ESCS_multiple_regression_6_0.png" />
</div>
</div>
<p>The five predictor variables that we’ll use are sum-scores measuring participants’ standings on the Big Five personality dimensions. The dimensions are: - O = Openness to experience - C = Conscientiousness - E = Extraversion - A = Agreeableness - N = Neuroticism</p>
<p>Here’s what our predictors look like:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">data</span><span class="p">[[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span><span class="s1">&#39;c&#39;</span><span class="p">,</span><span class="s1">&#39;e&#39;</span><span class="p">,</span><span class="s1">&#39;a&#39;</span><span class="p">,</span><span class="s1">&#39;n&#39;</span><span class="p">]]);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_ESCS_multiple_regression_8_0.png" src="../_images/notebooks_ESCS_multiple_regression_8_0.png" />
</div>
</div>
</div>
<div class="section" id="Specify-model-and-examine-priors">
<h2>Specify model and examine priors<a class="headerlink" href="#Specify-model-and-examine-priors" title="Permalink to this headline">¶</a></h2>
<p>We’re going to fit a pretty straightforward multiple regression model predicting drug use from all 5 personality dimension scores. It’s simple to specify the model using a familiar formula interface. Here we tell bambi to run two parallel Markov Chain Monte Carlo (MCMC) chains, each one drawing 2000 samples from the joint posterior distribution of all the parameters.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">bmb</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">fitted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="s1">&#39;drugs ~ o + c + e + a + n&#39;</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Sequential sampling (2 chains in 1 job)
INFO:pymc3:Sequential sampling (2 chains in 1 job)
NUTS: [drugs_sd, n, a, e, c, o, Intercept]
INFO:pymc3:NUTS: [drugs_sd, n, a, e, c, o, Intercept]
100%|██████████| 2000/2000 [00:29&lt;00:00, 68.95it/s]
100%|██████████| 2000/2000 [00:24&lt;00:00, 82.85it/s]
</pre></div></div>
</div>
<p>Great! But this is a Bayesian model, right? What about the priors?</p>
<p>If no priors are given explicitly by the user, then bambi chooses smart default priors for all parameters of the model based on the implied partial correlations between the outcome and the predictors. Here’s what the default priors look like in this case – the plots below show 1000 draws from each prior distribution:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">plot</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_ESCS_multiple_regression_13_0.png" src="../_images/notebooks_ESCS_multiple_regression_13_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Normal priors on the coefficients</span>
<span class="p">{</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">:</span><span class="n">x</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">args</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">terms</span><span class="o">.</span><span class="n">values</span><span class="p">()}</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;Intercept&#39;: {&#39;mu&#39;: array([2.21014664]), &#39;sd&#39;: array([7.49872428])},
 &#39;o&#39;: {&#39;mu&#39;: array([0]), &#39;sd&#39;: array([0.02706881])},
 &#39;c&#39;: {&#39;mu&#39;: array([0]), &#39;sd&#39;: array([0.03237049])},
 &#39;e&#39;: {&#39;mu&#39;: array([0]), &#39;sd&#39;: array([0.02957413])},
 &#39;a&#39;: {&#39;mu&#39;: array([0]), &#39;sd&#39;: array([0.03183624])},
 &#39;n&#39;: {&#39;mu&#39;: array([0]), &#39;sd&#39;: array([0.0264199])}}
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Uniform prior on the residual standard deviation</span>
<span class="n">model</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s1">&#39;sd&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">args</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;lower&#39;: array(0), &#39;upper&#39;: array(0.64877877)}
</pre></div></div>
</div>
<p>Some more info about the default prior distributions can be found in <a class="reference external" href="https://arxiv.org/abs/1702.01201">this technical paper</a>.</p>
<p>Notice the small SDs of the slope priors. This is due to the relative scales of the outcome and the predictors: remember from the plots above that the outcome, <code class="docutils literal notranslate"><span class="pre">drugs</span></code>, ranges from 1 to about 4, while the predictors all range from about 20 to 180 or so. So a one-unit change in any of the predictors – which is a trivial increase on the scale of the predictors – is likely to lead to a very small absolute change in the outcome. Believe it or not, these priors are actually quite wide on the
partial correlation scale!</p>
</div>
<div class="section" id="Examine-the-model-results">
<h2>Examine the model results<a class="headerlink" href="#Examine-the-model-results" title="Permalink to this headline">¶</a></h2>
<p>Let’s start with a pretty picture of the parameter estimates!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fitted</span><span class="o">.</span><span class="n">plot</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_ESCS_multiple_regression_20_0.png" src="../_images/notebooks_ESCS_multiple_regression_20_0.png" />
</div>
</div>
<p>The left panels show the marginal posterior distributions for all of the model’s parameters, which summarize the most plausible values of the regression coefficients, given the data we have now observed. These posterior density plots show two overlaid distributions because we ran two MCMC chains. The panels on the right are “trace plots” showing the sampling paths of the two MCMC chains as they wander through the parameter space.</p>
<p>A much more succinct (non-graphical) summary of the parameter estimates can be found like so:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fitted</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hpd0.95_lower</th>
      <th>hpd0.95_upper</th>
      <th>effective_n</th>
      <th>gelman_rubin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Intercept</th>
      <td>3.306788</td>
      <td>0.350794</td>
      <td>2.669343</td>
      <td>4.043687</td>
      <td>1125.481830</td>
      <td>1.001945</td>
    </tr>
    <tr>
      <th>n</th>
      <td>-0.001545</td>
      <td>0.001171</td>
      <td>-0.003944</td>
      <td>0.000732</td>
      <td>1436.056288</td>
      <td>1.000048</td>
    </tr>
    <tr>
      <th>o</th>
      <td>0.006041</td>
      <td>0.001189</td>
      <td>0.003704</td>
      <td>0.008393</td>
      <td>1524.267732</td>
      <td>0.999558</td>
    </tr>
    <tr>
      <th>a</th>
      <td>-0.012419</td>
      <td>0.001449</td>
      <td>-0.015050</td>
      <td>-0.009481</td>
      <td>1408.252642</td>
      <td>1.003233</td>
    </tr>
    <tr>
      <th>e</th>
      <td>0.003371</td>
      <td>0.001320</td>
      <td>0.000809</td>
      <td>0.005919</td>
      <td>1694.707896</td>
      <td>1.000497</td>
    </tr>
    <tr>
      <th>c</th>
      <td>-0.003806</td>
      <td>0.001503</td>
      <td>-0.006978</td>
      <td>-0.001149</td>
      <td>1356.351417</td>
      <td>0.999537</td>
    </tr>
    <tr>
      <th>drugs_sd</th>
      <td>0.592951</td>
      <td>0.016867</td>
      <td>0.559014</td>
      <td>0.624859</td>
      <td>1438.667543</td>
      <td>0.999888</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>When there are multiple MCMC chains, the default summary output includes some basic convergence diagnostic info (the effective MCMC sample sizes and the Gelman-Rubin “R-hat” statistics), although in this case it’s pretty clear from the trace plots above that the chains have converged just fine.</p>
</div>
<div class="section" id="Summarize-effects-on-partial-correlation-scale">
<h2>Summarize effects on partial correlation scale<a class="headerlink" href="#Summarize-effects-on-partial-correlation-scale" title="Permalink to this headline">¶</a></h2>
<p>Let’s grab the samples and put them in a format where we can easily work with them. We can do this really easily using the <code class="docutils literal notranslate"><span class="pre">to_df()</span></code> method of the fitted <code class="docutils literal notranslate"><span class="pre">MCMCResults</span></code> object.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">samples</span> <span class="o">=</span> <span class="n">fitted</span><span class="o">.</span><span class="n">to_df</span><span class="p">()</span>
<span class="n">samples</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Intercept</th>
      <th>n</th>
      <th>o</th>
      <th>a</th>
      <th>e</th>
      <th>c</th>
      <th>drugs_sd</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.440476</td>
      <td>-0.001174</td>
      <td>0.007033</td>
      <td>-0.012882</td>
      <td>0.001259</td>
      <td>-0.003642</td>
      <td>0.562209</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.073972</td>
      <td>-0.001429</td>
      <td>0.006784</td>
      <td>-0.013615</td>
      <td>0.003824</td>
      <td>-0.001993</td>
      <td>0.612918</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.455090</td>
      <td>-0.001833</td>
      <td>0.006368</td>
      <td>-0.014293</td>
      <td>0.003591</td>
      <td>-0.003459</td>
      <td>0.609669</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.555880</td>
      <td>-0.001826</td>
      <td>0.005902</td>
      <td>-0.015004</td>
      <td>0.003906</td>
      <td>-0.003191</td>
      <td>0.564443</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.425497</td>
      <td>-0.001180</td>
      <td>0.005629</td>
      <td>-0.015116</td>
      <td>0.003624</td>
      <td>-0.002424</td>
      <td>0.588922</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>It turns out that we can convert each regresson coefficient into a partial correlation by multiplying it by a constant that depends on (1) the SD of the predictor, (2) the SD of the outcome, and (3) the degree of multicollinearity with the set of other predictors. Two of these statistics are actually already computed and stores in the fitted model object, in a dictionary called <code class="docutils literal notranslate"><span class="pre">dm_statistics</span></code> (for design matrix statistics), because they are used internally. The others we will compute
manually.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># the names of the predictors</span>
<span class="n">varnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">]</span>

<span class="c1"># compute the needed statistics</span>
<span class="n">r2_x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">dm_statistics</span><span class="p">[</span><span class="s1">&#39;r2_x&#39;</span><span class="p">]</span>
<span class="n">sd_x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">dm_statistics</span><span class="p">[</span><span class="s1">&#39;sd_x&#39;</span><span class="p">]</span>
<span class="n">r2_y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">endog</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;drugs&#39;</span><span class="p">],</span>
                         <span class="n">exog</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">data</span><span class="p">[[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">varnames</span> <span class="k">if</span> <span class="n">p</span> <span class="o">!=</span> <span class="n">x</span><span class="p">]]))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">rsquared</span>
                  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">varnames</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">varnames</span><span class="p">)</span>
<span class="n">sd_y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;drugs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>

<span class="c1"># compute the products to multiply each slope with to produce the partial correlations</span>
<span class="n">slope_constant</span> <span class="o">=</span> <span class="n">sd_x</span><span class="p">[</span><span class="n">varnames</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">r2_x</span><span class="p">[</span><span class="n">varnames</span><span class="p">])</span><span class="o">**</span><span class="mf">.5</span> \
    <span class="o">/</span> <span class="n">sd_y</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">r2_y</span><span class="p">)</span><span class="o">**</span><span class="mf">.5</span>
<span class="n">slope_constant</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
o    32.392557
c    27.674284
e    30.305117
a    26.113299
n    34.130431
dtype: float64
</pre></div></div>
</div>
<p>Now we just multiply each sampled regression coefficient by its corresponding <code class="docutils literal notranslate"><span class="pre">slope_constant</span></code> to transform it into a sampled partial correlation coefficient.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pcorr_samples</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="n">varnames</span><span class="p">]</span> <span class="o">*</span> <span class="n">slope_constant</span>
<span class="n">pcorr_samples</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>o</th>
      <th>c</th>
      <th>e</th>
      <th>a</th>
      <th>n</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.227825</td>
      <td>-0.100794</td>
      <td>0.038152</td>
      <td>-0.336382</td>
      <td>-0.040077</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.219757</td>
      <td>-0.055156</td>
      <td>0.115887</td>
      <td>-0.355537</td>
      <td>-0.048786</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.206278</td>
      <td>-0.095714</td>
      <td>0.108840</td>
      <td>-0.373232</td>
      <td>-0.062565</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.191178</td>
      <td>-0.088297</td>
      <td>0.118359</td>
      <td>-0.391795</td>
      <td>-0.062335</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.182336</td>
      <td>-0.067090</td>
      <td>0.109831</td>
      <td>-0.394735</td>
      <td>-0.040289</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>And voilà! We now have a joint posterior distribution for the partial correlation coefficients. Let’s plot the marginal posterior distributions:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pcorr_samples</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">kde</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">.5</span><span class="p">])</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.lines.Line2D at 0x7efc94126f28&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_ESCS_multiple_regression_33_1.png" src="../_images/notebooks_ESCS_multiple_regression_33_1.png" />
</div>
</div>
<p>The means of these distributions serve as good point estimates of the partial correlations:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pcorr_samples</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a   -0.324302
c   -0.105328
n   -0.052734
e    0.102160
o    0.195684
dtype: float64
</pre></div></div>
</div>
<p>Naturally, these results are consistent with the OLS results. For example, we can see that these estimated partial correlations are roughly proportional to the t-statistics from the corresponding OLS regression:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">endog</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;drugs&#39;</span><span class="p">],</span> <span class="n">exog</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span>
    <span class="n">data</span><span class="p">[</span><span class="n">varnames</span><span class="p">]))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>drugs</td>      <th>  R-squared:         </th> <td>   0.176</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.169</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   25.54</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sat, 23 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>2.16e-23</td>
</tr>
<tr>
  <th>Time:</th>                 <td>20:00:37</td>     <th>  Log-Likelihood:    </th> <td> -536.75</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   604</td>      <th>  AIC:               </th> <td>   1086.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   598</td>      <th>  BIC:               </th> <td>   1112.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>
</tr>
<tr>
  <th>const</th> <td>    3.3037</td> <td>    0.360</td> <td>    9.188</td> <td> 0.000</td> <td>    2.598</td> <td>    4.010</td>
</tr>
<tr>
  <th>o</th>     <td>    0.0061</td> <td>    0.001</td> <td>    4.891</td> <td> 0.000</td> <td>    0.004</td> <td>    0.008</td>
</tr>
<tr>
  <th>c</th>     <td>   -0.0038</td> <td>    0.001</td> <td>   -2.590</td> <td> 0.010</td> <td>   -0.007</td> <td>   -0.001</td>
</tr>
<tr>
  <th>e</th>     <td>    0.0034</td> <td>    0.001</td> <td>    2.519</td> <td> 0.012</td> <td>    0.001</td> <td>    0.006</td>
</tr>
<tr>
  <th>a</th>     <td>   -0.0124</td> <td>    0.001</td> <td>   -8.391</td> <td> 0.000</td> <td>   -0.015</td> <td>   -0.010</td>
</tr>
<tr>
  <th>n</th>     <td>   -0.0015</td> <td>    0.001</td> <td>   -1.266</td> <td> 0.206</td> <td>   -0.004</td> <td>    0.001</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>10.273</td> <th>  Durbin-Watson:     </th> <td>   1.959</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.006</td> <th>  Jarque-Bera (JB):  </th> <td>   7.926</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.181</td> <th>  Prob(JB):          </th> <td>  0.0190</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.572</td> <th>  Cond. No.          </th> <td>3.72e+03</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.72e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.</div>
</div>
</div>
<div class="section" id="Relative-importance:-Which-predictors-have-the-strongest-effects-(defined-in-terms-of-partial-\eta^2)?">
<h2>Relative importance: Which predictors have the strongest effects (defined in terms of partial <span class="math notranslate nohighlight">\(\eta^2\)</span>)?<a class="headerlink" href="#Relative-importance:-Which-predictors-have-the-strongest-effects-(defined-in-terms-of-partial-\eta^2)?" title="Permalink to this headline">¶</a></h2>
<p>The partial <span class="math notranslate nohighlight">\(\eta^2\)</span> statistics for each predictor are just the squares of the partial correlation coefficients, so it’s easy to get posteriors on that scale too:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="p">(</span><span class="n">pcorr_samples</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">kde</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">.2</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">80</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.axes._subplots.AxesSubplot at 0x7efc94172080&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_ESCS_multiple_regression_40_1.png" src="../_images/notebooks_ESCS_multiple_regression_40_1.png" />
</div>
</div>
<p>With these posteriors we can ask: What is the probability that the partial <span class="math notranslate nohighlight">\(\eta^2\)</span> for Openness (yellow) is greater than the partial <span class="math notranslate nohighlight">\(\eta^2\)</span> for Conscientiousness (green)?</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="p">(</span><span class="n">pcorr_samples</span><span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">&gt;</span> <span class="n">pcorr_samples</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.928
</pre></div></div>
</div>
<p>For each predictor, what is the probability that it has the largest <span class="math notranslate nohighlight">\(\eta^2\)</span>?</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="p">(</span><span class="n">pcorr_samples</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">pcorr_samples</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a    0.992
o    0.008
dtype: float64
</pre></div></div>
</div>
<p>Agreeableness is clearly the strongest predictor of drug use among the Big Five personality traits, but it’s still not a particularly strong predictor in an absolute sense. Walter Mischel famously claimed that it is rare to see correlations between personality measurse and relevant behavioral outcomes exceed 0.3. In this case, the probability that the agreeableness partial correlation exceeds 0.3 is:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">pcorr_samples</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mf">.3</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.734
</pre></div></div>
</div>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="Bayesian_Frequentist_Tutorial.html" title="previous page">Bayesian/Frequentist Tutorial</a>
    <a class='right-next' id="next-link" href="shooter_crossed_random_ANOVA.html" title="next page">Bayesian Workflow Example (Police Officer’s Dilemma)</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, The developers of Bambi.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.0.1.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>